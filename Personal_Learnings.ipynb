{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Customer loans in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file serves as a tool for myself to revisit what I have done and the things that I have learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1-2: Initialise & run a class to extract the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialise the class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created a Python file to contain code for extraction - **db_utils.py**\n",
    "- Created a Class **RDSDatabase** which will be used for the extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Store Database Credentials**\n",
    "\n",
    "- Created a credentials.yaml file to store the database credentials provided by AiCore\n",
    "- Created a .gitignore file to keep the credentials secure and prevent them from being pushed to GitHub: \n",
    "\n",
    "    1. Create .gitignore file\n",
    "        - git init > git touch .gitignore > git nano .gitignore > \n",
    "    2. Add in credentials.yaml\n",
    "        - git add .gitignore \n",
    "    3. Commit to Github\n",
    "        - git commit -m \"Adding .gitignore to GitHub\" > git push origin main\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_credentials(filepath: str) -> dict:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        credentials = yaml.safe_load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in ErrorHandling controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_credentials(filepath: str) -> dict:\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            credentials = yaml.safe_load(f)\n",
    "            return credentials\n",
    "    except ExceptionError as e:\n",
    "        print(f\"Error loading credentials {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Initialise RDSDatabase Connector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initilising RDSDataseConnector taking the dictionary of credentials from above as a parameter\n",
    "- Setting \"*self.engine = None*\". Initilising it this way means that I am ensuring that the attributed are used only when they have valid values. It will be then set to a SQLAlchelmy engine object later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDSDatabaseConnector:\n",
    "    def __init__(self, credentials: dict):\n",
    "        self.credentials = credentials\n",
    "        self.engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Initialise SQLAlchemy Engine**\n",
    "\n",
    "- Defined method in RDSDatabaseConnector to set the engine to the SQLAlchemy Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def initialise_engine(self):\n",
    "        '''\n",
    "        Initialises a SQLAlchemy engine using the provided credentials\n",
    "        '''\n",
    "        try:\n",
    "            engine_url = (f\"postgresql://\n",
    "            {self.credentials['RDS_USER']}:{self.credentials['RDS_PASSWORD']}@{self.credentials['RDS_HOST']}/{self.credentials['RDS_DATABASE']}\")\n",
    "            self.engine=create_engine(engine_url)\n",
    "            print(\"SQLAlchemy engine initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing SQLAlchemy engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Extract data**\n",
    "- Created a method to extract data from the RDS Database and return it as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(self, query:str) -> pd.DataFrame:\n",
    "    if self.engine is None: \n",
    "        raise ValueError (\"Engine is not initialised. Call initialise_engine() first\")\n",
    "    return pd.read_sql(query,self.engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Create function to save the extracted data to a local file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv (self, data: pd.DataFrame, filename: str):\n",
    "    data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Disconnect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disconnect():\n",
    "    if self.engine:\n",
    "        self.engine.dispose()\n",
    "        print(\"SQLAlchemy engine connection is closed\")\n",
    "    else:\n",
    "        print(\"No active connection to close.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Call the method** \n",
    "- To ensure that this code is only run when called upon I included the line if __name__ = \"__main__\" \n",
    "- Then called the method to connect to the database and disconnect once finished saving the data to the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    credentials = load_credentials(\"credentials.yaml\")\n",
    "\n",
    "    connector = RDSDatabaseConnector(credentials)\n",
    "    connector.initialise_engine()\n",
    "    \n",
    "    query = \"SELECT * FROM loan_payments\"\n",
    "    data = connector.extract_data(query)\n",
    "\n",
    "    if not data.empty:\n",
    "        connector.save_to_csv(data,\"loan_payments.csv\")\n",
    "    \n",
    "    connector.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This milestone is set to gain a deeper understanding of the data and identify any patterns which might exist. I'll be: \n",
    "- Reviewing the data to identify any issues, such as missing or incorrectly formatted data. \n",
    "- Applying statistical techniques to gain insight on the data's distribution and apply visualisation techniques to identify patterns or trends in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert columns to the correct format within DataTransform Class** \n",
    "\n",
    "Are there any columns in the exisiting df that need amending? \n",
    "\n",
    "From the original data *df = pd.df = pd.read_csv(\"loan_payments.csv) > print(df.types)* I convert the following: \n",
    "- **term** = currently an object so convert to numberical but converting to an integer representing the number of months.\n",
    "- **issue_date, earliest_credit_line, last_payment_date, next_payment_date, last_credit_pull_date** = need to convert to datetime.\n",
    "- **employment_length** = convert to an integer, for <1 and 10+ change to 0 and 10 respectively. \n",
    "- **loan_status** = As it contains a limited number of unique values I convert to category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For employment date I need to extract the number from the full details given in the column :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column] = df[column].str.replace('< 1 year', '0')\n",
    "df[column] = df[column].str.replace('10+ years', '10')\n",
    "df[column] = df[column].str.extract(r'(\\d+)') #Explaination below\n",
    "df[column] = df[column].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For df[column].str.extract(r'(\\d+)')\n",
    "- **r** indicates that the string is a raw string, which means that the backslashes are treated as literal characters and not as escape characters\n",
    "- **\\d** matches any 0-9 digit\n",
    "- **+** means \"one or more\" of the preceeding element in this case is digits\n",
    "\n",
    "So **(r'(\\d+)')** matches one or more digits in the string and captures them as a group. The **str.extract** method returns a df with the extracted digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defined DataTransform Class and opened new ipynb to ensure all analysis is in one place**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This .ipynb is still used for my own personal understanding of what I learnt throughtout this project.\n",
    "\n",
    "- Analysis though is now found on loan_portfolio_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those columns that needed changing to datetime, if no date_format is provided, the method uses the default parsing behaviour of pd.to_datetime(). \n",
    "\n",
    "By explicitly stating the date format that we want to use we avoid potential errors that may arise later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def convert_to_datetime(self, column: str, date_format: str = None) -> pd.DataFrame:\n",
    "        if date_format:\n",
    "            self.df[column] = pd.to_datetime(self.df[column], format=date_format)\n",
    "        else: \n",
    "            self.df[column] = pd.to_datetime(self.df[column])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the method we have to specify the format wanted. In the data these columns are presented by 'Jan-2021 or May-2025' etc so abbreviated month name and total year. \n",
    "\n",
    "In Python to format this you have many options: \n",
    "- %B = Full month name \n",
    "- %b = abbreviated month name\n",
    "- %Y = four-digit year\n",
    "- %y = two-digit year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple columns to be tranformed you can do all in one go via: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def convert_multiple_to_datetime(self, columns: list, date_format: str = None) -> pd.DataFrame:\n",
    "        for column in columns: \n",
    "            if date_format:\n",
    "                self.df[column] = pd.to_datetime(self.df[column], format=date_format)\n",
    "            else: \n",
    "                self.df[column] = pd.to_datetime(self.df[column])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you call it with which will enable all the columns called out will be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert_to_datetime = [\n",
    "    'issue_date',\n",
    "    'earliest_credit_line',\n",
    "    'last_payment_date',\n",
    "    'next_payment_date',\n",
    "    'last_credit_pull_date'\n",
    "]\n",
    "transformer.convert_multiple_to_datetime(\n",
    "    columns_to_convert_to_datetime, date_format='%b-%Y'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Difference between total_payment & total_payment_inv\n",
    "\n",
    "- total_payment is from the borrower's perspective, representing the total amount they have paid, while \n",
    "- total_payment_inv is from the investor's perspective, representing the total amount they have received. \n",
    "\n",
    "total_payment_inv can be more insightful for someone analyzing loans from the perspective of a bank or financial institution. This is because total_payment_inv represents the total amount received by investors, which includes principal and interest payments. It provides a clear picture of the returns generated by the loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Difference between funded_amount & funded_amount_inv \n",
    "\n",
    "- funded_amount is from the borrower's perspective, representing the amount they have received, while \n",
    "- funded_amount_inv is from the investor's perspective, representing the amount they have invested.\n",
    "\n",
    "funded_amount_inv can be more insightful for someone analyzing loans from the perspective of a bank or financial institution, as it provides a clear picture of the investment made by the investors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Difference between out_prncp & out_prncp_inv\n",
    "\n",
    "- out_prncp is from the borrower's perspective, representing the amount they still need to repay, while \n",
    "- out_prncp_inv is from the investor's perspective, representing the amount they are still owed.\n",
    "\n",
    "out_prncp = Outstanding Principal\n",
    "\n",
    "out_prncp_inv can be more insightful for someone analyzing loans from the perspective of a bank or financial institution, as it provides a clear picture of the remaining investment that needs to be recovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 2**\n",
    "\n",
    "- Describe all columns in the DataFrame to check their data types\n",
    "- Extract statistical values: median, standard deviation and mean from the columns and the DataFrame\n",
    "- Count distinct values in categorical columns\n",
    "- Print out the shape of the DataFrame\n",
    "- Generate a count/percentage count of NULL values in each column\n",
    "- Any other methods you may find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I set up a new .py file **dataframe_info.py** to do host this code. It is best practice to seperate classes to ensure ease of following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataFrameInfo:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def describe_columns(self) -> pd.DataFrame:\n",
    "        return self.df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the statistical values i.e. mean, median and standard deviation this\n",
    "can only be extracted from numberical columns so we must put a condition\n",
    "in place so that the class only looks at the relevant columns:\n",
    "\n",
    "- numberic_cols=self.df.select_dtypes(include=['number']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_statistical_values(self) -> pd.DataFrame:\n",
    "        numberic_cols=self.df.select_dtypes(include=['number']).columns\n",
    "        return self.df[numberic_cols].agg(['median','std','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical values provide a summary of the data in the DataFrame, giving insights into the distribution and variability of the data. Here's what each typically show:\n",
    "\n",
    "1. **Median**: The median is the middle value of the data when it is sorted in ascending order. It is a measure of central tendency that is less affected by outliers compared to the mean. The median helps to understand the typical value in the data.\n",
    "\n",
    "2. **Standard Deviation (std)**: The standard deviation measures the amount of variation or dispersion in the data.     \n",
    "- A low standard deviation indicates that the data points are close to the mean, while \n",
    "- a high standard deviation indicates that the data points are spread out over a wider range. \n",
    "\n",
    "    It helps you understand the consistency of the data.\n",
    "\n",
    "3. **Mean**: The mean is the average of the data, calculated by summing all the values and dividing by the number of data points. It is a measure of central tendency that gives an idea of the overall level of the data. However, it **can be affected by outliers.**\n",
    "\n",
    "By examining these statistical values, you can gain a better understanding of the characteristics of the data, such as its central tendency, variability, and distribution. This information is useful for identifying patterns, detecting anomalies, and making informed decisions based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from our data were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                 id     member_id   loan_amount  funded_amount  \\\n",
    "median  7.084590e+06  8.709873e+06  12000.000000   12000.000000   \n",
    "std     9.571362e+06  1.031281e+07   8082.196709    8019.017599   \n",
    "mean    7.621797e+06  8.655350e+06  13333.076100   13229.509117   \n",
    "\n",
    "        funded_amount_inv       term   int_rate  instalment  \\\n",
    "median       11300.000000  36.000000  13.160000  347.150000   \n",
    "std           8099.473527  15.826533   4.392893  238.920012   \n",
    "mean         12952.622979  38.857111  13.507328  400.013953   \n",
    "\n",
    "        employment_length    annual_inc  ...  total_payment_inv  \\\n",
    "median           6.000000  61000.000000  ...        9835.830000   \n",
    "std              3.649479  51589.339577  ...        8363.508506   \n",
    "mean             5.690749  72220.848249  ...       11788.946618   \n",
    "\n",
    "        total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
    "median      7644.920000    1734.640000            0.000000    0.000000   \n",
    "std         6958.124264    2581.657345            6.215792  630.843636   \n",
    "mean        9407.048589    2577.757101            0.901512   93.501288   \n",
    "\n",
    "        collection_recovery_fee  last_payment_amount  \\\n",
    "median                 0.000000           562.670000   \n",
    "std                  120.193950          5323.801675   \n",
    "mean                  10.859057          3130.706393   \n",
    "\n",
    "        collections_12_mths_ex_med  mths_since_last_major_derog  policy_code  \n",
    "median                    0.000000                    42.000000          1.0  \n",
    "std                       0.070990                    21.052360          0.0  \n",
    "mean                      0.004208                    42.253634          1.0  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary in the findings: \n",
    "1. Loan_amount = Suggests typical loan amount is around £12,000, there are some loans with significantly higher amounts, leading to a higher mean than standard deviation. \n",
    "2. Term = The median term is 36 months, with a mean of 38.86 months and a standard deviation of 15.83. This indicates that most loans have a term of around 36 months, but there are some loans with longer terms. \n",
    "3. Interest Rate (int_rate) = The median interest rate is 13.16%, with a mean of 13.50 and standard deviation of 4.39%. This suggests that while the typical interest rate is around 13.6%, there is some variation in the interest rates offered.\n",
    "4. Annual Income (annual_inc) = Suggests typical income is around £61,000, with a mean of £72,220.85 and standard deviation of £51,589.34. Whilst typical Annual Income is around £61,000 there are some borrowers with significantly higher incomes, leading to a higher mean than standard deviation. \n",
    "5. Total Payment (total_payment_inv) = The median total payment is £9,835.83, with a mean of £11,788.95 and standard deviation of £8,363.51. This suggests that although the typical total payment is around £9,835.83, there are some loans with significantly higher total payments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making Informed Decisions**\n",
    "\n",
    "- Loan Approval: Understanding the typical loan amounts, terms, and interest rates can help you set criteria for loan approval.\n",
    "\n",
    "- Risk Assessment: Analyzing the variation in annual income and total payments can help you assess the risk associated with different borrowers.\n",
    "\n",
    "- Product Offerings: Identifying patterns in loan data can help you tailor your product offerings to meet the needs of your customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then further DataFrameInfo useful tools: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def count_distinct_values(self) -> pd.DataFrame:\n",
    "        categorical_columns = self.df.select_dtypes(\n",
    "            include=['category','object']).columns\n",
    "        return self.df[categorical_columns].nunique()\n",
    "    \n",
    "    def print_shape(self) -> tuple:\n",
    "        return self.df.shape\n",
    "\n",
    "    def count_null_values(self) -> pd.DataFrame:\n",
    "        null_counts = self.df.isnull().sum()\n",
    "        null_percentage = round((self.df.isnull().sum() / len(self.df)) * 100,2)\n",
    "        return pd.DataFrame({'null_count': null_counts, 'null_percentage': null_percentage})\n",
    "\n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        return self.df.describe()\n",
    "\n",
    "    def get_correlation_matrix(self) -> pd.DataFrame:\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the Distinct Values in Category Columns allows us to: \n",
    "- Modeling: In ML, knowing the distinct values is crucial for encoding categorical variables and ensuring that the model can handle them correctly. \n",
    "- Understand the Data: Diversity of the categories in the present data\n",
    "- Cleaning: Can help you spot inconsistencies or errors in the data, for example unexpected or misspelled categories, you can clean and standardize the data. \n",
    "- Analysis & Visualisations: Create bar/pie chart for different categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count NULL values - is explained in Task 3 (below) \n",
    "\n",
    "- Describe - Why do we do describe as well as statistical values? \n",
    "\n",
    "    Statistical values allows you to extract only the statistical values you're interested in whereas summary provides a broad overview. Describe provides a summary of the central tendency, dispersion, and shape of the dataset’s distribution for numeric columns. This is a quick way to get an overview of your data. \n",
    "\n",
    "    We have both to help with validation purposes, comparing the results from both methods can help validate findings and ensure consistency in analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Removing NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- funded_amount: 3,007 null values (5.54%)\n",
    "- int_rate: 5,169 null values (9.53%)\n",
    "- employment_length: 2,118 null values (3.91%)\n",
    "- mths_since_last_delinq: 31,002 null values (57.17%)\n",
    "- mths_since_last_record: 48,050 null values (88.60%)\n",
    "- last_payment_date: 73 null values (0.13%)\n",
    "- next_payment_date: 32,608 null values (60.13%)\n",
    "- last_credit_pull_date: 7 null values (0.01%)\n",
    "- collections_12_mths_ex_med: 51 null values (0.09%)\n",
    "- mths_since_last_major_derog: 46,732 null values (86.17%)\n",
    "\n",
    "So which columns to drop?\n",
    "- Anything above 50% drop: \n",
    "    - mths_since_last_delinq\n",
    "    - mths_since_last_record\n",
    "    - next_payment_date\n",
    "    - mths_since_last_major_derog\n",
    "- Impute mean or median on the remaining columns\n",
    "    - From our data we can see that the data contains outliers/extreme values, so we will use median as it is less affected by outliers. \n",
    "    - However in the code we have added in a line so that if we were to use this again and we chose to use mean we can.\n",
    "    The median helps preserve the central tendency of the data without being influenced by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def impute_missing_values(self, strategy: str = 'median') -> pd.DataFrame:\n",
    "        for column in self.df.columns:\n",
    "            if self.df[column].isnull().sum() > 0:\n",
    "                if self.df[column].dtype in ['float64','int64']:\n",
    "                    if strategy == 'median':\n",
    "                        self.df[column].fillna(self.df[column].median())\n",
    "                    elif strategy == 'mean':\n",
    "                        self.df[column].fillna(self.df[column].mean())\n",
    "                else:\n",
    "                    self.df[column] = self.df[column].fillna(self.df[column].mode()[0])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then call it using *transformer.impute_missing_values(strategy='median')*\n",
    "\n",
    "- Note that for Non-Numeric Columns the code fills missing values with the mode (most frequent value).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify skewed columns.\n",
    "- Determine threshold, over which column will be considered as skewed.\n",
    "- Visualise this using Plotter class.\n",
    "- Perform transformations to determine which gives biggest reduction of skew: \n",
    "    - Log\n",
    "    - Square Root\n",
    "    - BoxCox\n",
    "- Apply the identified transformations on the columns\n",
    "- Visualise to check the results to ensure transformations have improved skewness of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Skewed Columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberical_cols = cleaned_df.select_dtypes(include=['float64','int64'])\n",
    "skewness = numberical_cols.skew().abs()\n",
    "print(\"Skewness of columns:\\n\", skewness)\n",
    "\n",
    "Skewness of columns:\n",
    " id                             2.370336\n",
    "member_id                      2.205422\n",
    "loan_amount                    0.805259\n",
    "funded_amount                  0.821787\n",
    "funded_amount_inv              0.813927\n",
    "term                           0.707703\n",
    "int_rate                       0.412032\n",
    "instalment                     0.996981\n",
    "employment_length              0.115188\n",
    "annual_inc                     8.711831\n",
    "dti                            0.189420\n",
    "delinq_2yrs                    5.370002\n",
    "inq_last_6mths                 3.248918\n",
    "open_accounts                  1.059282\n",
    "total_accounts                 0.779014\n",
    "out_prncp                      2.356426\n",
    "out_prncp_inv                  2.356848\n",
    "total_payment                  1.267891\n",
    "total_payment_inv              1.256197\n",
    "total_rec_prncp                1.261015\n",
    "total_rec_int                  2.204322\n",
    "total_rec_late_fee            13.184305\n",
    "recoveries                    14.589793\n",
    "collection_recovery_fee       27.636843\n",
    "last_payment_amount            2.499381\n",
    "collections_12_mths_ex_med    20.252780\n",
    "policy_code                    0.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Threshold** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following guidelines: \n",
    "\n",
    "- < 0.5 **Low skewness** : Data fairly symmetrical no transformation needed. \n",
    "\n",
    "- 0.5 - 1.0 **Moderate Skewness** : Consider transformation if impacts analysis.\n",
    "\n",
    "- '>'  1.0 **High Skewness** : Transformation recommended to reduce skewness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the skewed columns are identified, you should perform transformations on these columns to determine which transformation results in the biggest reduction in skew. Create the the method to transform the columns in your DateFrameTransform class.\n",
    "\n",
    "The three methods we will use to compare against each other are: \n",
    "1. Log transformations: \n",
    "    - Reduces skewness by compressing the range of values. It is particularly useful for data with a long right tail (positive skew). For example: \n",
    "\n",
    "        ```python\n",
    "        Values  Log_Transformed\n",
    "        0       1         0.693147\n",
    "        1      10         2.397895\n",
    "        2     100         4.615121\n",
    "        3    1000         6.908755\n",
    "        4   10000         9.210440\n",
    "\n",
    "        ```\n",
    "\n",
    "2. Square Root transformation\n",
    "    - Reduces skewness by compressing the range of values, but less aggressively than the log transformation. It is useful for moderately skewed data.\n",
    "\n",
    "    ```python\n",
    "\n",
    "        Values  Sqrt_Transformed\n",
    "        0       1          1.000000\n",
    "        1      10          3.162278\n",
    "        2     100         10.000000\n",
    "        3    1000         31.622777\n",
    "        4   10000        100.000000\n",
    "    ```\n",
    "\n",
    "3. Boxcox Transformation \n",
    "    - A more flexible transformation that can handle a variety of data distributions. It requires all input values to be positive and can stabilize variance and make the data more normally distributed.\n",
    "\n",
    "        ```python\n",
    "        Values  BoxCox_Transformed\n",
    "        0       1            0.000000\n",
    "        1      10            1.000000\n",
    "        2     100            2.000000\n",
    "        3    1000            3.000000\n",
    "        4   10000            4.000000\n",
    "        ```\n",
    "\n",
    "##### Summary: \n",
    "\n",
    "**Log Transformation:** Compresses the range of values, useful for highly skewed data.\n",
    "\n",
    "**Square Root Transformation:** Compresses the range of values, useful for moderately skewed data.\n",
    "\n",
    "**Box-Cox Transformation:** Flexible transformation that stabilizes variance and makes data more normally distributed, requires positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_skewed_cols(self, columns:list) -> pd.DataFrame:\n",
    "        for column in columns:\n",
    "            print(f\"\\nProcessing column: {column}\")\n",
    "\n",
    "            if (self.df[column] <= 0).any():\n",
    "                self.df[column] = self.df[column] - self.df[column].min() + 1\n",
    "\n",
    "            #Apply log transformation\n",
    "            log_transformed = np.log1p(self.df[column])\n",
    "            log_skewness = log_transformed.skew()\n",
    "            print(f\"Log skewness for {column}: {log_skewness}\")\n",
    "\n",
    "            # Apply square root transformation\n",
    "            sqrt_transformed = np.sqrt(self.df[column])\n",
    "            sqrt_skewness = sqrt_transformed.skew()\n",
    "            print(f\"Sqrt skewness for {column}: {sqrt_skewness}\")\n",
    "\n",
    "            # Apply Box-Cox transformation (requires positive values)\n",
    "            try:\n",
    "                boxcox_transformed, _ = boxcox(self.df[column])\n",
    "                boxcox_skewness = pd.Series(boxcox_transformed).skew()\n",
    "                print(f\"Box-Cox skewness for {column}: {boxcox_skewness}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error applying Box-Cox transformation to column {column}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Determine the best transformation\n",
    "            transformations = {\n",
    "                'log': log_skewness,\n",
    "                'sqrt': sqrt_skewness,\n",
    "                'boxcox': boxcox_skewness\n",
    "            }\n",
    "            best_transformation = min(transformations, key=transformations.get)\n",
    "            print(f\"\\nBest transformation for {column}: {best_transformation}\")\n",
    "\n",
    "            # Apply the best transformation\n",
    "            if best_transformation == 'log':\n",
    "                self.df[column] = log_transformed\n",
    "            elif best_transformation == 'sqrt':\n",
    "                self.df[column] = sqrt_transformed\n",
    "            elif best_transformation == 'boxcox':\n",
    "                self.df[column] = boxcox_transformed\n",
    "\n",
    "        return self.df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking it down:\n",
    "\n",
    "``` python\n",
    "if (self.df[column] <= 0).any():\n",
    "                self.df[column] = self.df[column] - self.df[column].min() + 1\n",
    "```\n",
    "\n",
    "This is checking if any value in the columns are <=0 and if so shifts the entire column by subtracting the minimum value and adding 1 to ensure all values are positive. This is because BoxCox cannot transform negative numbers. \n",
    "\n",
    "Shifted Values = Original Value - Minimum Value + 1\n",
    "\n",
    "```Python\n",
    "If we had the following values: \n",
    "-5\n",
    "-3\n",
    "0\n",
    "2\n",
    "\n",
    "This would then become: \n",
    "(-5) - (-5) + 1 = 1\n",
    "(-3) - (-5) + 1 = 3\n",
    "(0) - (-5) + 1 = 6\n",
    "(2) - (-5) + 1 = 8\n",
    "```\n",
    "\n",
    "\"shift entire column\" = adjusting all the values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply log transformation\n",
    "log_transformed = np.log1p(self.df[column])\n",
    "log_skewness = log_transformed.skew()\n",
    "print(f\"Log skewness for {column}: {log_skewness}\")\n",
    "\n",
    "# Apply square root transformation\n",
    "sqrt_transformed = np.sqrt(self.df[column])\n",
    "sqrt_skewness = sqrt_transformed.skew()\n",
    "print(f\"Sqrt skewness for {column}: {sqrt_skewness}\"\n",
    "\n",
    "# Apply Box-Cox transformation (requires positive values)\n",
    "try:\n",
    "    boxcox_transformed, _ = boxcox(self.df[column])\n",
    "    boxcox_skewness = pd.Series(boxcox_transformed).skew()\n",
    "    print(f\"Box-Cox skewness for {column}: {boxcox_skewness}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error applying Box-Cox transformation to column {column}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Transformation: This applies the log transformation to the column using np.log1p, which is equivalent to np.log(1 + x). It then calculates the skewness of the transformed column and prints it.\n",
    "\n",
    "Square Root Transformation: This applies the square root transformation to the column using np.sqrt. It then calculates the skewness of the transformed column and prints it.\n",
    "\n",
    "Box-Cox Transformation: \n",
    "- To ensure that the code works without crashing we have included Error Handling in this part of the code.\n",
    "- ValueError will capture if any of the data is non-positive and will print a message indicating that the Box-Cox transformation has failed for the specific column and provides an error message. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best transformation\n",
    "transformations = {\n",
    "    'log': log_skewness,\n",
    "    'sqrt': sqrt_skewness,\n",
    "    'boxcox': boxcox_skewness\n",
    "    }\n",
    "\n",
    "best_transformation = min(transformations, key=transformations.get)\n",
    "print(f\"\\nBest transformation for {column}: {best_transformation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a dictionary with the skewness values for each transformation. It then finds the transformation with the lowest skewness (i.e., the best transformation) and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the best transformation\n",
    "if best_transformation == 'log':\n",
    "    self.df[column] = log_transformed\n",
    "elif best_transformation == 'sqrt':\n",
    "    self.df[column] = sqrt_transformed\n",
    "elif best_transformation == 'boxcox':\n",
    "    self.df[column] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This applies the best transformation to the column based on the lowest skewness value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reasons behind reducing skewness are:\n",
    "\n",
    "- Statistical Assumptions: Many statistical methods assume that the data is normally distributed. Reducing skewness can help meet these assumptions and improve the performance of these methods.\n",
    "\n",
    "- Model Performance: In machine learning, the choice of transformation can impact model performance. It's often a good idea to experiment with different transformations and evaluate their impact on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these via Plotly?\n",
    " \n",
    " - Updated the __init__ method to include both original_df and transformed_df which will allow us to compare the two when calling\n",
    " - Updated null_values to include a transformed part, this hasn't changed my documentation as I am still able to show the original data (new_df) in one graph. Still initialising with plotter = Plotter(original_df=df, transformed_df=transformed_df) but then having new_df as both arguments. Similarly when the data is cleaned used the same in both! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    '''\n",
    "    Class to visualise insights from the data\n",
    "    '''\n",
    "    def __init__ (self, original_df:pd.DataFrame, transformed_df:pd.DataFrame):\n",
    "        self.original_df = original_df\n",
    "        self.transformed_df = transformed_df\n",
    "\n",
    "    def plot_null_values(self):\n",
    "        #Plot null values in the original DataFrame\n",
    "        null_counts_original = self.original_df.isnull().sum()\n",
    "        null_counts_original = null_counts_original[null_counts_original > 0]\n",
    "        null_df_original= pd.DataFrame({'Columns': null_counts_original.index, 'Null Values': null_counts_original.values})\n",
    "        fig = px.bar(null_df_original, x='Columns', y='Null Values', title='Null Values in Each Column')\n",
    "        fig.show()\n",
    "\n",
    "        # Plot null values in the transformed DataFrame\n",
    "        null_counts_transformed = self.transformed_df.isnull().sum()\n",
    "        null_counts_transformed = null_counts_transformed[null_counts_transformed > 0]\n",
    "        null_df_transformed = pd.DataFrame({'Columns': null_counts_transformed.index, 'Null Values': null_counts_transformed.values})\n",
    "        fig = px.bar(null_df_transformed, x='Columns', y='Null Values', title='Null Values in Each Column (Transformed)')\n",
    "        \n",
    "\n",
    "    def plot_histogram(self, column: str):\n",
    "        # Plot original data\n",
    "        fig = px.histogram(self.original_df, x=column, nbins=30, title=f'Histogram of {column} (Original)')\n",
    "        fig.update_layout(xaxis_title=column, yaxis_title='Frequency')\n",
    "        fig.show()\n",
    "\n",
    "        # Plot transformed data\n",
    "        fig = px.histogram(self.transformed_df, x=column, nbins=30, title=f'Histogram of {column} (Transformed)')\n",
    "        fig.update_layout(xaxis_title=column, yaxis_title='Frequency')\n",
    "        fig.show()\n",
    "\n",
    "    def plot_boxplot(self, column: str):\n",
    "        # Plot original data\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Box(y=self.original_df[column], name=f'{column} (Original)'))\n",
    "\n",
    "        # Plot transformed data\n",
    "        fig.add_trace(go.Box(y=self.transformed_df[column], name=f'{column} (Transformed)'))\n",
    "\n",
    "        fig.update_layout(title=f'Boxplot of {column} (Original and Transformed)', yaxis_title='Values')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IQR Method**: Identifies outliers based on the interquartile range.\n",
    "\n",
    "**Z-score Method**: Identifies outliers based on the number of standard deviations from the mean.\n",
    "\n",
    "**Combined Approach**: Uses both methods to provide a comprehensive outlier detection.\n",
    "\n",
    "By using both IQR and Z-score methods, you can effectively identify and handle outliers in your dataset, improving the quality and accuracy of your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def remove_outliers(self, columns: list, method: str = 'both'):\n",
    "        for column in columns:\n",
    "            if method == 'IQR'or method == 'both':\n",
    "                Q1 = self.df[column].quantile(0.25)\n",
    "                Q3 = self.df[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                self.df = self.df[(self.df[column] >= lower_bound) & (self.df[column] <= upper_bound)]\n",
    "                #Here the method is filtering through to include only rows where the column value are within the calculated bounds.\n",
    "            elif method == 'Z-score' or method == 'both':\n",
    "                self.df = self.df[(np.abs(stats.zscore(self.df[column])) < 3)]\n",
    "                #Returns the modified dataframe with the outliers removed.\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have decided to do both so that all outliers can be removed. You can do one or the other by statinf 'IQR' or 'Z-Score' when calling the class i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = transformer.remove_outliers(columns=numerical_columns, method='both')\n",
    "\n",
    "#OR single methods: \n",
    "\n",
    "clean_df = transformer.remove_outliers(columns=numerical_columns, method='IQR')\n",
    "clean_df = transformer.remove_outliers(columns=numerical_columns, method='Z-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Dropping overly correlated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly correlated columns in a dataset can lead to multicollinearity issues, which can affect the accuracy and interpretability of models built on the data. In this task, you will identify highly correlated columns and remove them to improve the quality of the data.\n",
    "\n",
    "\n",
    "**Step 1:** First compute the correlation matrix for the dataset and visualise it.\n",
    "\n",
    "\n",
    "Step 2: Identify which columns are highly correlated. You will need to decide on a correlation threshold and to remove all columns above this threshold.\n",
    "\n",
    "\n",
    "Step 3: Decide which columns can be removed based on the results of your analysis.\n",
    "\n",
    "\n",
    "Step 4: Remove the highly correlated columns from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1 - Visualise it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a new class as the plotter class took in two arguments which is no longer needed for this. New class called DataVisualiser. I initiated it and then started the function below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(self, columns:pd.DataFrame):\n",
    "    '''\n",
    "    Method to plot the correlation matrix of the given DataFrame\n",
    "    '''\n",
    "    # Convert non-numeric columns to numeric and drop columns with non-numeric values\n",
    "    correlation_matrix = columns.corr() \n",
    "\n",
    "    fig = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\", color_continuous_scale='RdBu_r')\n",
    "    fig.update_layout(title='Correlation Matrix', width=800, height=800)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the numerical columns that we produced earlier, this function now will take the columns as an argument and return all the numerical columns with correlation. Below is how we would then call it, using only numerical columns and the newly clean_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_visualiser\n",
    "numerical_columns = clean_df.select_dtypes(include=['float64','int64'])\n",
    "correlation = data_visualiser.DataVisualiser(clean_df)\n",
    "correlation.plot_correlation_matrix(numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If however we wanted to open in a webbrowser for ease of use rather than all on the VScode we can use the below code, which will then open a pop up window with the matrix on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os     \n",
    "\n",
    "     if save_as_html:\n",
    "            fig.write_html(file_name)\n",
    "            webbrowser.open('file://' + os.path.realpath(file_name))\n",
    "        else:\n",
    "            pio.show(fig, renderer='browser') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking it down: \n",
    "\n",
    "- if `save_as_html` checks if the `save_to_html` parameter is set to `True` when calling the method.\n",
    "- `fig.write_html(file_name)` saves the Plotly figure as an HTML specified with the specified `file_name` (again when calling the method)\n",
    "- `webbrowser.open('file://' + os.path.realpath(file_name))` This opens the saved HTML file in a new browser window using the `webbrowser` module that had to import specifically for this stage. \n",
    "    - `os.path.realpath(file_name)` ensures that the full path to the file is used.\n",
    "- else `save_to_html` is `False`\n",
    "    -`pio.show(fig,renderer='browser')` will open the plot in a new browser window if the save_as_html is False.\n",
    "\n",
    "You'd call this by:\n",
    "\n",
    "`correlation.plot_correlation_matrix(numerical_columns, save_as_html=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 & 3 -  Identify which columns are highly correlated. You will need to decide on a correlation threshold and to remove all columns above this threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used correlation of 0.8 to be the threshold. It's the most common but is not a hard rule. If you want to be more stringent, change to 0.9 or if you want to capture more relationships move to 0.7. This is changeable by updating the amount in the function and when calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def find_highly_correlated_pairs(self, correlation_matrix: pd.DataFrame, threshold: float = 0.8):\n",
    "        '''\n",
    "        Function to find pairs of highly correlated features\n",
    "        '''\n",
    "        highly_correlated_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                    highly_correlated_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "        return highly_correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above:\n",
    "\n",
    "- Outer Loop:\n",
    "    -  This loop iterates over the columns of the correlation matrix. The variable `i` represents the index of the current column. \n",
    "    - `len(correlation_matrix.columns)` tells us the number of columns within the correlation matrix.\n",
    "\n",
    "- Inner Loop:\n",
    "    - This loop iterates over the columns up to the current column `i`. The variable `j` represents the index of the current column in the inner loop. \n",
    "    - By using `range(i)` the inner loop ensures that each pair of columns is considered only once, avoiding duplicate pairs and self pairs (i.e. where `i` == `j`)\n",
    "\n",
    "- Check Correction:\n",
    "    - `if abs(correlation_matrix.iloc[i, j]) > threshold:` checks if the absolute value of the correlation coefficent between columns `i` and `j` is greater than the specified threshold.\n",
    "    - `correlation_matrix.iloc[i, j]` accesses the correlation coefficient between the `i`-th and `j`-th columns of the correlation matrix.\n",
    "    - `abs`is used to consider both positive and negative correlations.\n",
    "\n",
    "\n",
    "- Append Highly Correlated Pairs:\n",
    "    - If correlation coefficient exceeds the threshold, the pair of columns and their correlation coefficient are appended to the `highly_correlated_pairs` list.\n",
    "    - `correlation_matrix.columns[i]` and `correlation_matrix.columns[j]` give the names of the columns.\n",
    "    - `correlation_matrix.iloc[i, j]` gives the correlation coefficient between the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features: member_id and id - Correlation: 0.9900472991039773\n",
    "Features: funded_amount and loan_amount - Correlation: 0.9948368389540044\n",
    "Features: funded_amount_inv and loan_amount - Correlation: 0.9718686232142686\n",
    "Features: funded_amount_inv and funded_amount - Correlation: 0.978275163802146\n",
    "Features: instalment and loan_amount - Correlation: 0.9884455412722591\n",
    "Features: instalment and funded_amount - Correlation: 0.9941440249097828\n",
    "Features: instalment and funded_amount_inv - Correlation: 0.9714405982843544\n",
    "Features: out_prncp_inv and out_prncp - Correlation: 0.9999999945687313\n",
    "Features: total_payment and loan_amount - Correlation: 0.8308019310827159\n",
    "Features: total_payment and funded_amount - Correlation: 0.8327576941881051\n",
    "Features: total_payment and funded_amount_inv - Correlation: 0.8049919014718379\n",
    "Features: total_payment and instalment - Correlation: 0.8343072964617597\n",
    "Features: total_payment_inv and loan_amount - Correlation: 0.8085626991307507\n",
    "Features: total_payment_inv and funded_amount - Correlation: 0.8126288846993878\n",
    "Features: total_payment_inv and funded_amount_inv - Correlation: 0.8347148948857773\n",
    "Features: total_payment_inv and instalment - Correlation: 0.8134371791332475\n",
    "Features: total_payment_inv and total_payment - Correlation: 0.9655279710498574\n",
    "Features: total_rec_prncp and loan_amount - Correlation: 0.801046862781626\n",
    "Features: total_rec_prncp and funded_amount - Correlation: 0.8022933302032733\n",
    "Features: total_rec_prncp and total_payment - Correlation: 0.9914675928619078\n",
    "Features: total_rec_prncp and total_payment_inv - Correlation: 0.9568986422416129\n",
    "Features: mths_since_last_major_derog and mths_since_last_delinq - Correlation: 0.8217320198486451"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down the above: \n",
    "\n",
    "- **member_id**\n",
    "    - **id** have a very high correlation of 0.9900, suggesting these identifiers are almost identical.\n",
    "\n",
    "    - ACTION - Remove `id`\n",
    "\n",
    "\n",
    "- **funded_amount** \n",
    "    - **loan_amount** - Using linear regression we can see that the relationship between loan_amount & funded_amount shows a good fit (with a R-squared of 0.93) and with the high correlation of 0.96 we can drop funded_amount and keep loan_amount.\n",
    "    - **funded_amount_inv** - Given the high correlation and the near-perfect inverse relationship between funded_amount_inv and loan_amount, you can confidently drop funded_amount_inv from the dataset, as it is a derived feature and won't provide any additional insights or value.\n",
    "\n",
    "    - ACTION - Remove `funded_amount` & `funded_amount_inv`\n",
    "\n",
    "- **instalment**\n",
    "    - **loan_amount** Since I have already dropped funded_amount & funded_amount_inv then it seems logical to remove instalment.\n",
    "\n",
    "    - ACTION - Remove `instalment`\n",
    "\n",
    "- **out_prncp_inv**\n",
    "    - **out_prncp** are identical, having looked into detail about the difference of these two the out_prncp is more useful for the company than the out_prncp_inv. As out_prncp represents the portion of the outstanding principal that is owed to investors it is not as important to the company as out_prncp\n",
    "\n",
    "    - ACTION - Remove `out_prncp_inv`\n",
    "\n",
    "- **total_payment**\n",
    "    - **total_payment_inv** - total amount paid to investors. Not necessary for this analysis I don't think and given the high correlation not needed. \n",
    "    - **total_rec_prncp** - total principal recieved by the lender, similary to the above, not as necessary as `total_payment`\n",
    "    - `total_payment` provides a comprehensive view of the total amount paid by the borrower, which includes both principal and interest.\n",
    "\n",
    "    - ACTION - Remove `total_payment_inv` & `total_rec_prncp`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just simply did it like this, then reprinted and put this updated df through the matrix builder to show the differences made from dropping these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['id','funded_amount', 'funded_amount_inv', 'instalment', 'out_prncp_inv', 'total_payment_inv', 'total_rec_prncp']\n",
    "transformer = DataFrameTransform(clean_df)\n",
    "updated_df=transformer.drop_highly_correlated_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_numerical_cols=updated_df.select_dtypes(include=['float64','int64'])\n",
    "new_correlation = data_visualiser.DataVisualiser(updated_df)\n",
    "new_correlation.plot_correlation_matrix(new_numerical_cols, save_as_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactoring:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataTransform**\n",
    "- changed `-> pd.DataFrame` to `-> 'DataTransform'`:\n",
    "    - Why? - By returning from methods, you enable method chaining. This allows you to apply multiple tansformations in a concise and readable way for example you could: \n",
    "\n",
    "       ```python\n",
    "        updated_df = \n",
    "        transformer.convert_to_int('column1')\n",
    "        .convert_to_category('column2')\n",
    "        .drop_columns(columns_to_drop)\n",
    "        .get_dataframe()\n",
    "        ```\n",
    "    - Returning this also keeps the transformations encapsulated within the class. Meaning that you can maintain the state of the DataFrame and add more methods to the class without affecting the external code.  \n",
    "\n",
    "    - Whereas if you returned `pd.DataFrame` it provides direct access to the modified DataFrame after each transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisation**\n",
    "- Changed some of the code for clarity from `correlation_matrix = columns.corr()` to \n",
    "\n",
    "    ``` python\n",
    "        selected_df = self.df[columns]\n",
    "        correlation_matrix = selected_df.corr() \n",
    "    ```\n",
    "\n",
    "    - This ensures that only relevant columns are included in the correlation matrix, avoiding unnecessary computations for unrelated columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrameInfo** \n",
    "- Added in docstrings to describe the purpose and parameters of all the functions and classes. \n",
    "- Changed `def count_distinct_values(self) -> pd.DataFrame:` to be `-> pd.Series` because the output of this method is a series of distinct value counts for categorical columns, not a DataFrame.\n",
    "    - Series = one-dimentsional array with an index, which makes it suitable for representing counts of distinct values for each categorical column \n",
    "    - DataFrame = two-dimensional, tabular data structure, which is more appropriate for representing multiple columns and rows of data. \n",
    "\n",
    "- def print_shape(self) -> tuple: Because the shape of a DataFrame is naturally represented as a tuple. \n",
    "    - Tuple is a data structure in Python that allows you to store an ordered collection of items. Immutable. Heterogeneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
